# ğŸ§  MCP Memory Chatbot using Groq LLM

This project is an **interactive AI chatbot** built using **MCP (Model Context Protocol)** with **conversation memory enabled**. It uses **Groq's LLaMA-4 model** for fast inference and integrates with an **Airbnb MCP Server** for tool-based reasoning.

The chatbot supports:

* Multi-turn conversations
* Built-in memory (context persistence)
* Tool calling via MCP
* Interactive CLI-based chat

---

## ğŸš€ Features

* ğŸ”— **MCP Clientâ€“Server Architecture** (Airbnb MCP Server)
* ğŸ§  **Conversation Memory Enabled**
* âš¡ **Groq LLM Integration** (LLaMA-4 Maverick)
* ğŸ› ï¸ **Tool Invocation with Structured JSON**
* ğŸ§¹ Commands to clear memory and exit chat
* ğŸ–¥ï¸ Fully **async** Python implementation

---

## ğŸ—ï¸ Project Architecture

```
User (CLI)
   â†“
MCPAgent (Memory + Reasoning)
   â†“
Groq LLM (LLaMA-4)
   â†“
MCP Client
   â†“
Airbnb MCP Server (Tools)
```

---

## ğŸ“ Project Structure

```
MCP-Memory-Chatbot/
â”‚
â”œâ”€â”€ main.py                # Entry point (async chat loop)
â”œâ”€â”€ mcp_config.json        # MCP server configuration
â”œâ”€â”€ .env                   # Environment variables (API keys)
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # Project documentation
```

---

## âš™ï¸ Requirements

* Python **3.11**
* Groq API Key
* Airbnb MCP Server

### Python Libraries

* `mcp_use`
* `langchain-groq`
* `python-dotenv`
* `asyncio`

---

## ğŸ” Environment Setup

Create a `.env` file in the project root:

```
GROQ_API=your_groq_api_key_here
```

---

## ğŸ§© MCP Configuration

Ensure `mcp_config.json` is correctly set up to connect to the **Airbnb MCP Server**.

Example:

```
{
    "mcpServers": {
      "airbnb": {
        "command": "npx",
        "args": [
          "-y",
          "@openbnb/mcp-server-airbnb"
        ]
      }
    }
  }
```

---

## â–¶ï¸ How to Run

1. Install dependencies:

```
pip install -r requirements.txt
```

2. Start the chatbot:

```
python main.py
```

---

## ğŸ’¬ Chat Commands

| Command         | Description               |
| --------------- | ------------------------- |
| `exit` / `quit` | End the chat              |
| `clear`         | Clear conversation memory |

---

## ğŸ§  Memory Behavior

* The chatbot **remembers previous messages** within a session
* Memory can be cleared manually using the `clear` command
* Memory helps in contextual follow-up questions

---

## ğŸ¤– LLM Details

* **Provider:** Groq
* **Model:** `meta-llama/llama-4-maverick-17b-128e-instruct`
* **Inference:** Ultra-low latency via Groq

---

## ğŸ› ï¸ Key Code Highlights

* Uses `MCPAgent` with `memory_enabled=True`
* Enforces **strict JSON typing** for tool calls
* Graceful session cleanup using async context

---

## ğŸ”® Future Enhancements

* Web UI using Streamlit or React
* Persistent memory (vector DB)
* Authentication & user sessions
* Multi-agent MCP workflows

---

## ğŸ“Œ Use Case

This project is ideal for:

* MCP learning & experimentation
* Tool-augmented LLM systems
* Enterprise AI assistants
* Research on conversational memory

---

## ğŸ‘¤ Author

**Raviteja**
AI / Data Science Enthusiast
Focused on GenAI, MCP, LangGraph, and LLM Systems

---

## ğŸ“„ License

This project is for educational and research purposes.
